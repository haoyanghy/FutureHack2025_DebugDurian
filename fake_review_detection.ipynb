{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88378845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Conv1D, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "486aa0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text.lower())\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adaca717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_dim=5000, embedding_dim=128, input_length=100):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim, embedding_dim, input_length=input_length),\n",
    "        Conv1D(64, 5, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.build((None, input_length)) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08176d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_lstm(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        MaxPooling1D(5),\n",
    "        Dropout(0.5),\n",
    "        LSTM(128),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       epochs=5,\n",
    "                       batch_size=64)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_accuracy': history.history['accuracy'][-1],\n",
    "        'val_accuracy': history.history['val_accuracy'][-1],\n",
    "        'train_loss': history.history['loss'][-1],\n",
    "        'val_loss': history.history['val_loss'][-1]\n",
    "    }\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9effaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Learning\n",
    "def incremental_training(new_data_path):\n",
    "    model = tf.keras.models.load_model('models/cnn_lstm.h5')\n",
    "    tokenizer = joblib.load('models/tokenizer.pkl')\n",
    "\n",
    "    df_new = pd.read_csv(new_data_path)\n",
    "    X_new = df_new['text']\n",
    "    y_new = df_new['label']\n",
    "\n",
    "    X_new_seq = tokenizer.texts_to_sequences(X_new)\n",
    "    X_new_pad = pad_sequences(X_new_seq, maxlen=100)\n",
    "\n",
    "    model.fit(X_new_pad, y_new, epochs=2, batch_size=32, verbose=1)\n",
    "\n",
    "    model.save('models/cnn_lstm.h5')\n",
    "    print(\"Model updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d834f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.7420 - loss: 0.4758 - val_accuracy: 0.8660 - val_loss: 0.3018\n",
      "Epoch 2/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 26ms/step - accuracy: 0.8963 - loss: 0.2388 - val_accuracy: 0.8724 - val_loss: 0.2936\n",
      "Epoch 3/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - accuracy: 0.9251 - loss: 0.1786 - val_accuracy: 0.8734 - val_loss: 0.3096\n",
      "Epoch 4/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 25ms/step - accuracy: 0.9460 - loss: 0.1280 - val_accuracy: 0.8741 - val_loss: 0.3509\n",
      "Epoch 5/5\n",
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - accuracy: 0.9577 - loss: 0.0977 - val_accuracy: 0.8715 - val_loss: 0.3803\n",
      "\n",
      "=== Model Performance ===\n",
      "   train_accuracy  val_accuracy  train_loss  val_loss\n",
      "0        0.953871        0.8715    0.107672  0.380323\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS = 5000\n",
    "MAX_LEN = 100\n",
    "\n",
    "df = pd.read_csv('data/processed_data.csv')\n",
    "df['processed_text'] = df['text'].apply(preprocess) \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(df['processed_text'])  \n",
    "    \n",
    "sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "cnn_lstm, metrics = train_cnn_lstm(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "print(\"\\n=== Model Performance ===\")\n",
    "print(pd.DataFrame([metrics]))\n",
    "    \n",
    "print(\"\\nSaving model...\")\n",
    "joblib.dump(tokenizer, 'models/tokenizer.pkl')\n",
    "cnn_lstm.save('models/cnn_lstm.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
